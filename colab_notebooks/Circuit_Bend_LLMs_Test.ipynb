{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "1jXFm0PrjajomYLYBHFm9nr9nTs7r86Jw",
      "authorship_tag": "ABX9TyM1gop30ENFuyYar/OR70C4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/designingEmergence/CircuitBendingTests/blob/main/colab_notebooks/Circuit_Bend_LLMs_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Circuit Bending LLMs - Test\n",
        "\n"
      ],
      "metadata": {
        "id": "pik_w-5ON3x_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Circuit Bending\n",
        "Circuit bending is the art of taking an old, sound making toy and hacking the circuits to produce a different sound than expected. To circuit bend something, you open it up, find the circuit board that is responsible for making the sound, lick your finger and touch the resistors and components of the circuit while pressing the sound-activating buttons. The effect you get is a glitchy, altered version of the original sound, since you are short circuiting the transistors and changing the resistance of the analog circuit with your wet finger. Circuit bending is a great way to get unexpected and unique sounds by essentially glitching the circuit. Hmm.. glitching the circuit… Aha! I wonder if I can circuit bend an AI model to make it behave “drunk”.\n",
        "\n",
        "### Circuit Bending AI - The Theory\n",
        "\n",
        "In theory, there are a lot of parallels between circuit bending old toys and AI models. Just like, an old toy, you can open up an AI model and expose the weights of its neural network, which are essentially the circuit board of the model. These weights define how the AI model operates. The reason Stable Diffusion produces an image of an apple when you prompt “Photo of Apple, close up” is because the weights have been set to a precise configuration by hours and hours of training. You can then lick your metaphorical finger and “bend” the circuit by changing the weights of the model, which will produce glitchy and unexpected outputs!\n",
        "\n",
        "### Purpose of this notebook\n",
        "\n",
        "Here we are testing if its possible to manipulate the weights of LLMs and run the manipulated version of the LLM.\n",
        "\n",
        "### Pre-Requisites\n",
        "\n",
        "- Download an LLM. Here I am using Lllama3.2-1B. You can download the model directly from the [llama hugging face space](https://huggingface.co/meta-llama/Llama-3.2-1B/tree/main)\n",
        "- Upload this model to either Google Drive or directly to the runtime (although the model is large so I recommend Google Drive)\n",
        "\n",
        "Note: You can use any LLM that is hugging face transformer compatible, it doesn't have to be Llama.\n"
      ],
      "metadata": {
        "id": "_dT81Q6dveMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "tc4wmfpsOGaG",
        "outputId": "fe8e7bbc-7fd3-4fdc-9763-ff48906dc28e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-748a26749645>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install PyTorch (if not already installed)\n",
        "# Colab usually comes with PyTorch pre-installed, but you can update it if needed.\n",
        "\n",
        "!pip install torch transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyU7V1dgOnjn",
        "outputId": "bcc20ba4-2844-40f0-9e2f-2b5a4753575c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Import libraries\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "-OOjjkNEOvn7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Set up paths\n",
        "\n",
        "drive_model_path = '/content/drive/MyDrive/Projects/Circuit_Bending_AI/models/Llama-3.2-1B-hf'\n",
        "modified_model_path = '/content/drive/MyDrive/Projects/Circuit_Bending_AI/models/Llama-3.2-1B-hf_mod'"
      ],
      "metadata": {
        "id": "8kty8lmMPHdp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define functions\n",
        "\n",
        "# Load model and tokenizer from google drive\n",
        "def load_model_and_tokenizer_from_drive(path):\n",
        "  print(f\"Loading model and tokenizer from {path}....\")\n",
        "  model = AutoModelForCausalLM.from_pretrained(path)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "  print(\"Model and tokenizer loaded succesfully\")\n",
        "  return tokenizer, model\n",
        "\n",
        "# Bend model weights\n",
        "def bend_weights(model, noise_factor=0.01, layer_query=None):\n",
        "  print(\"Bending model weights...\")\n",
        "   # Create a copy of the state_dict to prevent changes to the original model\n",
        "  state_dict = model.state_dict()\n",
        "  modified_state_dict = {k: v.clone() for k, v in state_dict.items()}\n",
        "\n",
        "  # Normalize `layer_query` to a list for consistent handling\n",
        "  if isinstance(layer_query, str):\n",
        "    layer_query = [layer_query]\n",
        "\n",
        "  # Add noise to the weights of the first attention layer\n",
        "  for layer_name, weights in modified_state_dict.items():\n",
        "    #print(f\"layer: {layer_name}\")\n",
        "    if layer_query is None or any(query in layer_name for query in layer_query):\n",
        "      print(f\"Bending {layer_name}...\")\n",
        "      noise = torch.randn(weights.size()) * noise_factor\n",
        "      modified_state_dict[layer_name] += noise\n",
        "\n",
        "  from transformers import AutoConfig, LlamaForCausalLM\n",
        "  # Create a new model with the modified weights\n",
        "  config = AutoConfig.from_pretrained(model.config.name_or_path)\n",
        "  modified_model = LlamaForCausalLM(config)\n",
        "  modified_model.load_state_dict(modified_state_dict)\n",
        "\n",
        "  print(\"Weights bent succesfully!\")\n",
        "  return modified_model\n",
        "\n",
        "# Save modified model to google drive\n",
        "def save_model_to_drive(model, path):\n",
        "  print(f\"Saving modified model to {path}...\")\n",
        "  model.save_pretrained(path)\n",
        "  print(\"Bent model saved succesfully!\")\n",
        "\n",
        "def test_model(tokenizer, model, prompt=\"The quick brown fox\", max_length=50, temperature=1.0, top_k=50, top_p=0.9):\n",
        "  try:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        do_sample=True\n",
        "    )\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Output: {result}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error during testing: {e}\")"
      ],
      "metadata": {
        "id": "MnF9A9aPTpMo"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load Model\n",
        "\n",
        "tokenizer, model = load_model_and_tokenizer_from_drive(drive_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QczBUzaanHm",
        "outputId": "e77dda63-6a86-463c-facb-eabefe61b3b1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer from /content/drive/MyDrive/Projects/Circuit_Bending_AI/models/Llama-3.2-1B-hf....\n",
            "Model and tokenizer loaded succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 (optional): Test original model\n",
        "\n",
        "test_model(tokenizer,model, prompt=\"Once upon a time\", max_length=50, temperature=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDtrWeQHwDdM",
        "outputId": "3963df32-6ac0-4058-f2b4-cf478dc2ada6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Once upon a time, there was a man who was very rich. He had a lot of money, and he was very happy with his life. But one day, he decided to take a trip to the city. He was going to see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Modify model\n",
        "\n",
        "modified_model = bend_weights(model, noise_factor=0.5, layer_query='embed_tokens.weight')\n",
        "\n",
        "test_model(tokenizer, modified_model, prompt=\"Once upon a time\", max_length=50, temperature=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP_f0pHE_ggb",
        "outputId": "68a7beaa-d329-4a9b-ee8f-78063a5a6ac9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bending model weights...\n",
            "Bending model.embed_tokens.weight...\n",
            "Weights bent succesfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Once upon a time, there was a man who was very rich. He had a lot of money, and he was very happy with his life. But one day, he decided to take a trip to the city. He was going to visit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Save Modified model\n",
        "\n",
        "name = \"test-1\"\n",
        "save_model_to_drive(modified_model, modified_model_path + name)"
      ],
      "metadata": {
        "id": "ptG3vve7_wl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}