{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/designingEmergence/CircuitBendingTests/blob/main/colab_notebooks/llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac9adb4",
      "metadata": {
        "id": "3ac9adb4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/llama_2_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "368686b4-f487-4dd4-aeff-37823976529d",
      "metadata": {
        "id": "368686b4-f487-4dd4-aeff-37823976529d"
      },
      "source": [
        "# LlamaCPP\n",
        "\n",
        "In this short notebook, we show how to use the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) library with LlamaIndex.\n",
        "\n",
        "In this notebook, we use the [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) model, along with the proper prompt formatting.\n",
        "\n",
        "Note that if you're using a version of `llama-cpp-python` after version `0.1.79`, the model format has changed from `ggmlv3` to `gguf`. Old model files like the used in this notebook can be converted using scripts in the [`llama.cpp`](https://github.com/ggerganov/llama.cpp) repo. Alternatively, you can download the GGUF version of the model above from [huggingface](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF).\n",
        "\n",
        "By default, if model_path and model_url are blank, the `LlamaCPP` module will load llama2-chat-13B in either format depending on your version.\n",
        "\n",
        "## Installation\n",
        "\n",
        "To get the best performance out of `LlamaCPP`, it is recomended to install the package so that it is compilied with GPU support. A full guide for installing this way is [here](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal).\n",
        "\n",
        "Full MACOS instructions are also [here](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/).\n",
        "\n",
        "In general:\n",
        "- Use `CuBLAS` if you have CUDA and an NVidia GPU\n",
        "- Use `METAL` if you are running on an M1/M2 MacBook\n",
        "- Use `CLBLAST` if you are running on an AMD/Intel GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff273be",
      "metadata": {
        "id": "aff273be"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-llms-llama-cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a33749",
      "metadata": {
        "id": "40a33749"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7927630-0044-41fb-a8a6-8dc3d2adb608",
      "metadata": {
        "id": "e7927630-0044-41fb-a8a6-8dc3d2adb608"
      },
      "source": [
        "## Setup LLM\n",
        "\n",
        "The LlamaCPP llm is highly configurable. Depending on the model being used, you'll want to pass in `messages_to_prompt` and `completion_to_prompt` functions to help format the model inputs.\n",
        "\n",
        "Since the default model is llama2-chat, we use the util functions found in [`llama_index.llms.llama_utils`](https://github.com/jerryjliu/llama_index/blob/main/llama_index/llms/llama_utils.py).\n",
        "\n",
        "For any kwargs that need to be passed in during initialization, set them in `model_kwargs`. A full list of available model kwargs is available in the [LlamaCPP docs](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__init__).\n",
        "\n",
        "For any kwargs that need to be passed in during inference, you can set them in `generate_kwargs`. See the full list of [generate kwargs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__call__).\n",
        "\n",
        "In general, the defaults are a great starting point. The example below shows configuration with all defaults.\n",
        "\n",
        "As noted above, we're using the [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) model in this notebook which uses the `ggmlv3` model format. If you are running a version of `llama-cpp-python` greater than `0.1.79`, you can replace the `model_url` below with `\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b27895",
      "metadata": {
        "id": "59b27895"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439960c5",
      "metadata": {
        "id": "439960c5"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2640c7a4",
      "metadata": {
        "id": "2640c7a4"
      },
      "outputs": [],
      "source": [
        "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20",
      "metadata": {
        "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20"
      },
      "outputs": [],
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url=model_url,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 1},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445453b1",
      "metadata": {
        "id": "445453b1"
      },
      "source": [
        "We can tell that the model is using `metal` due to the logging!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e2e6a78-7e5d-4915-bcbf-6087edb30276",
      "metadata": {
        "id": "5e2e6a78-7e5d-4915-bcbf-6087edb30276"
      },
      "source": [
        "## Start using our `LlamaCPP` LLM abstraction!\n",
        "\n",
        "We can simply use the `complete` method of our `LlamaCPP` LLM abstraction to generate completions given a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cfaf34c-0348-415e-98bb-83f782d64fe9",
      "metadata": {
        "id": "5cfaf34c-0348-415e-98bb-83f782d64fe9"
      },
      "outputs": [],
      "source": [
        "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9038f7d7",
      "metadata": {
        "id": "9038f7d7"
      },
      "source": [
        "We can use the `stream_complete` endpoint to stream the response as itâ€™s being generated rather than waiting for the entire response to be generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b059409-cd9d-4651-979c-03b3943e94af",
      "metadata": {
        "id": "7b059409-cd9d-4651-979c-03b3943e94af"
      },
      "outputs": [],
      "source": [
        "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
        "for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7617600",
      "metadata": {
        "id": "f7617600"
      },
      "source": [
        "## Query engine set up with LlamaCPP\n",
        "\n",
        "We can simply pass in the `LlamaCPP` LLM abstraction to the `LlamaIndex` query engine as usual.\n",
        "\n",
        "But first, let's change the global tokenizer to match our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ff0c0b",
      "metadata": {
        "id": "d8ff0c0b"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import set_global_tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "set_global_tokenizer(\n",
        "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c6f564",
      "metadata": {
        "id": "d4c6f564"
      },
      "outputs": [],
      "source": [
        "# use Huggingface embeddings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d485f1e",
      "metadata": {
        "id": "5d485f1e"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"../../../examples/paul_graham_essay/data\"\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55c33cd",
      "metadata": {
        "id": "c55c33cd"
      },
      "outputs": [],
      "source": [
        "# create vector store index\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07659c8",
      "metadata": {
        "id": "e07659c8"
      },
      "outputs": [],
      "source": [
        "# set up query engine\n",
        "query_engine = index.as_query_engine(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e095c5",
      "metadata": {
        "id": "64e095c5"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}